#!/usr/bin/env python3
"""
Load processed data into Solr for the optimization demo.

This script loads the product data and sets up the Solr collection
for testing and optimization.
"""

import os
import sys
import json
import requests
import time
from pathlib import Path
from typing import Dict, List
import logging

# Add the project root to the Python path
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SolrDataLoader:
    """Loads processed data into Solr collection."""
    
    def __init__(self, 
                 solr_url: str = "http://localhost:8983/solr",
                 collection_name: str = "ecommerce_products",
                 data_dir: Path = None):
        self.solr_url = solr_url
        self.collection_name = collection_name
        self.data_dir = data_dir or PROJECT_ROOT / "demo" / "data"
        self.processed_dir = self.data_dir / "processed"
        
    def check_solr_connection(self) -> bool:
        """Check if Solr is running and accessible."""
        try:
            response = requests.get(f"{self.solr_url}/admin/info/system", timeout=10)
            response.raise_for_status()
            logger.info(f"‚úÖ Solr is accessible at {self.solr_url}")
            return True
        except requests.exceptions.RequestException as e:
            logger.error(f"‚ùå Cannot connect to Solr at {self.solr_url}: {e}")
            logger.error("   Make sure Solr is running. Try: cd demo/docker-setup && ./setup.sh")
            return False
    
    def check_collection_exists(self) -> bool:
        """Check if the target collection exists."""
        try:
            response = requests.get(
                f"{self.solr_url}/admin/collections",
                params={"action": "LIST", "wt": "json"},
                timeout=10
            )
            response.raise_for_status()
            collections = response.json().get("collections", [])
            
            if self.collection_name in collections:
                logger.info(f"‚úÖ Collection '{self.collection_name}' exists")
                return True
            else:
                logger.error(f"‚ùå Collection '{self.collection_name}' not found")
                logger.error(f"   Available collections: {collections}")
                logger.error("   Run Docker setup first: cd demo/docker-setup && ./setup.sh")
                return False
                
        except requests.exceptions.RequestException as e:
            logger.error(f"‚ùå Error checking collections: {e}")
            return False
    
    def clear_existing_data(self) -> bool:
        """Clear existing data from the collection."""
        try:
            logger.info(f"üßπ Clearing existing data from '{self.collection_name}'...")
            
            # Delete all documents
            delete_data = {"delete": {"query": "*:*"}}
            response = requests.post(
                f"{self.solr_url}/{self.collection_name}/update",
                json=delete_data,
                headers={"Content-Type": "application/json"},
                timeout=30
            )
            response.raise_for_status()
            
            # Commit the deletion
            commit_data = {"commit": {}}
            response = requests.post(
                f"{self.solr_url}/{self.collection_name}/update",
                json=commit_data,
                headers={"Content-Type": "application/json"},
                timeout=30
            )
            response.raise_for_status()
            
            logger.info("‚úÖ Existing data cleared")
            return True
            
        except requests.exceptions.RequestException as e:
            logger.error(f"‚ùå Error clearing data: {e}")
            return False
    
    def load_products(self, batch_size: int = 100) -> bool:
        """Load product data into Solr in batches."""
        products_file = self.processed_dir / "products.json"
        
        if not products_file.exists():
            logger.error(f"‚ùå Products file not found: {products_file}")
            logger.error("   Run data download first: python demo/scripts/download_data.py")
            return False
        
        logger.info(f"üì• Loading products from {products_file}...")
        
        try:
            with open(products_file) as f:
                products = json.load(f)
            
            logger.info(f"   Found {len(products):,} products to load")
            
            # Load in batches
            total_loaded = 0
            for i in range(0, len(products), batch_size):
                batch = products[i:i + batch_size]
                
                if self._load_product_batch(batch):
                    total_loaded += len(batch)
                    if total_loaded % 1000 == 0:
                        logger.info(f"   Loaded {total_loaded:,} / {len(products):,} products...")
                else:
                    logger.error(f"‚ùå Failed to load batch starting at index {i}")
                    return False
            
            # Final commit
            if self._commit_changes():
                logger.info(f"‚úÖ Successfully loaded {total_loaded:,} products")
                return True
            else:
                logger.error("‚ùå Failed final commit")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Error loading products: {e}")
            return False
    
    def _load_product_batch(self, products: List[Dict]) -> bool:
        """Load a batch of products into Solr."""
        try:
            # Prepare documents for Solr
            solr_docs = []
            for product in products:
                # Convert product to Solr document format
                doc = {
                    "id": product["id"],
                    "product_title": product["product_title"],
                    "product_description": product["product_description"],
                    "product_bullet_point": product["product_bullet_point"] if isinstance(product["product_bullet_point"], list) else [product["product_bullet_point"]],
                    "product_brand": product["product_brand"],
                    "product_color": product["product_color"],
                    "product_locale": product["product_locale"],
                    "category": product.get("category", ""),
                    "price": product.get("price", 0.0)
                }
                solr_docs.append(doc)
            
            # Send to Solr
            response = requests.post(
                f"{self.solr_url}/{self.collection_name}/update",
                json=solr_docs,
                headers={"Content-Type": "application/json"},
                timeout=30
            )
            response.raise_for_status()
            
            return True
            
        except requests.exceptions.RequestException as e:
            logger.error(f"‚ùå Error loading batch: {e}")
            if hasattr(e, 'response') and e.response is not None:
                logger.error(f"   Response: {e.response.text}")
            return False
    
    def _commit_changes(self) -> bool:
        """Commit changes to make them visible."""
        try:
            commit_data = {"commit": {}}
            response = requests.post(
                f"{self.solr_url}/{self.collection_name}/update",
                json=commit_data,
                headers={"Content-Type": "application/json"},
                timeout=30
            )
            response.raise_for_status()
            return True
            
        except requests.exceptions.RequestException as e:
            logger.error(f"‚ùå Error committing changes: {e}")
            return False
    
    def verify_data_loaded(self) -> bool:
        """Verify that data was loaded correctly with comprehensive quality checks."""
        try:
            logger.info("üîç Verifying loaded data...")
            
            # Check document count
            response = requests.get(
                f"{self.solr_url}/{self.collection_name}/select",
                params={"q": "*:*", "rows": "0", "wt": "json"},
                timeout=10
            )
            response.raise_for_status()
            
            result = response.json()
            doc_count = result["response"]["numFound"]
            
            logger.info(f"   üìä Total documents in collection: {doc_count:,}")
            
            if doc_count == 0:
                logger.error("‚ùå No documents found in collection")
                return False
            
            # Data quality checks
            quality_passed = self._run_data_quality_checks()
            
            # Test multiple sample queries to ensure search functionality
            test_queries = ["laptop", "smartphone", "headphones", "camera"]
            working_queries = 0
            
            for query in test_queries:
                try:
                    response = requests.get(
                        f"{self.solr_url}/{self.collection_name}/select",
                        params={
                            "q": query,
                            "df": "_text_",
                            "rows": "5",
                            "wt": "json",
                            "fl": "id,product_title,product_brand"
                        },
                        timeout=10
                    )
                    response.raise_for_status()
                    
                    result = response.json()
                    found_docs = result["response"]["numFound"]
                    
                    if found_docs > 0:
                        working_queries += 1
                        sample_doc = result["response"]["docs"][0]
                        logger.info(f"   ‚úÖ Query '{query}': {found_docs} results (e.g., {sample_doc.get('product_title', 'N/A')})")
                    else:
                        logger.warning(f"   ‚ö†Ô∏è  Query '{query}': No results found")
                        
                except Exception as e:
                    logger.warning(f"   ‚ùå Query '{query}': Error - {e}")
            
            search_success_rate = (working_queries / len(test_queries)) * 100
            logger.info(f"   üìä Search success rate: {search_success_rate:.0f}% ({working_queries}/{len(test_queries)} queries)")
            
            # Overall verification result
            if quality_passed and search_success_rate >= 75:
                logger.info("‚úÖ Data verification completed successfully")
                return True
            elif search_success_rate >= 50:
                logger.warning("‚ö†Ô∏è  Data verification passed with warnings")
                logger.warning("   Some queries may not work optimally")
                return True
            else:
                logger.error("‚ùå Data verification failed")
                logger.error("   Search functionality is severely impaired")
                return False
            
        except requests.exceptions.RequestException as e:
            logger.error(f"‚ùå Error verifying data: {e}")
            return False
    
    def _run_data_quality_checks(self) -> bool:
        """Run comprehensive data quality checks."""
        try:
            logger.info("   üîç Running data quality checks...")
            
            # Get field statistics and facets
            response = requests.get(
                f"{self.solr_url}/{self.collection_name}/select",
                params={
                    "q": "*:*",
                    "rows": "0",
                    "wt": "json",
                    "facet": "true",
                    "facet.field": ["product_brand", "product_locale"],
                    "stats": "true",
                    "stats.field": ["product_title", "product_description"]
                },
                timeout=15
            )
            response.raise_for_status()
            
            result = response.json()
            total_docs = result["response"]["numFound"]
            
            # Check field coverage
            stats = result.get("stats", {}).get("stats_fields", {})
            title_stats = stats.get("product_title", {})
            desc_stats = stats.get("product_description", {})
            
            title_count = title_stats.get("count", 0)
            desc_count = desc_stats.get("count", 0)
            
            title_coverage = (title_count / total_docs) * 100 if total_docs > 0 else 0
            desc_coverage = (desc_count / total_docs) * 100 if total_docs > 0 else 0
            
            # Check diversity
            facets = result.get("facet_counts", {}).get("facet_fields", {})
            brand_facets = facets.get("product_brand", [])
            locale_facets = facets.get("product_locale", [])
            
            unique_brands = len(brand_facets) // 2 if brand_facets else 0
            unique_locales = len(locale_facets) // 2 if locale_facets else 0
            
            # Quality assessment
            quality_issues = []
            
            if title_coverage < 90:
                quality_issues.append(f"Low title coverage: {title_coverage:.1f}%")
            if desc_coverage < 70:
                quality_issues.append(f"Low description coverage: {desc_coverage:.1f}%")
            if unique_brands < 5:
                quality_issues.append(f"Few unique brands: {unique_brands}")
            if unique_locales < 1:
                quality_issues.append(f"No locale diversity: {unique_locales}")
            
            # Report results
            logger.info(f"   üìä Field coverage: titles {title_coverage:.1f}%, descriptions {desc_coverage:.1f}%")
            logger.info(f"   üìä Data diversity: {unique_brands} brands, {unique_locales} locales")
            
            if quality_issues:
                logger.warning(f"   ‚ö†Ô∏è  Data quality issues found:")
                for issue in quality_issues:
                    logger.warning(f"      ‚Ä¢ {issue}")
                
                # Return true if issues are minor
                critical_issues = [i for i in quality_issues if "Low title coverage" in i or "Few unique brands" in i]
                return len(critical_issues) == 0
            else:
                logger.info("   ‚úÖ Data quality checks passed")
                return True
                
        except Exception as e:
            logger.warning(f"   ‚ö†Ô∏è  Error running quality checks: {e}")
            return True  # Don't fail entirely on quality check errors
    
    def create_demo_queries_file(self) -> bool:
        """Create a demo queries file that works with the loaded data."""
        try:
            logger.info("üìù Creating demo queries file...")
            
            # Load the generated queries
            queries_file = self.processed_dir / "queries.csv"
            if not queries_file.exists():
                logger.error(f"‚ùå Queries file not found: {queries_file}")
                return False
            
            import csv
            
            # Read queries and create a demo-friendly format
            demo_queries = []
            with open(queries_file, 'r') as f:
                reader = csv.DictReader(f)
                for i, row in enumerate(reader):
                    if i < 20:  # Take first 20 queries for demo
                        demo_queries.append({
                            "query_id": row["query_id"],
                            "query_text": row["query"],
                            "description": f"Demo query: {row['query']}"
                        })
            
            # Save demo queries
            demo_queries_file = self.data_dir / "demo_queries.json"
            with open(demo_queries_file, 'w') as f:
                json.dump(demo_queries, f, indent=2)
            
            logger.info(f"‚úÖ Created demo queries file: {demo_queries_file}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error creating demo queries: {e}")
            return False
    
    def show_loading_summary(self):
        """Show a summary of what was loaded."""
        try:
            # Get collection stats
            response = requests.get(
                f"{self.solr_url}/{self.collection_name}/select",
                params={"q": "*:*", "rows": "0", "wt": "json", "facet": "true", "facet.field": "product_brand"},
                timeout=10
            )
            response.raise_for_status()
            
            result = response.json()
            doc_count = result["response"]["numFound"]
            
            # Get brand distribution
            brand_facets = result.get("facet_counts", {}).get("facet_fields", {}).get("product_brand", [])
            brand_counts = {}
            for i in range(0, len(brand_facets), 2):
                if i + 1 < len(brand_facets):
                    brand_counts[brand_facets[i]] = brand_facets[i + 1]
            
            print("\n" + "="*60)
            print("üéâ DATA LOADING SUMMARY")
            print("="*60)
            print(f"Collection: {self.collection_name}")
            print(f"Total Products: {doc_count:,}")
            print(f"Solr URL: {self.solr_url}")
            print(f"\nTop Brands:")
            for brand, count in sorted(brand_counts.items(), key=lambda x: x[1], reverse=True)[:10]:
                print(f"  {brand}: {count:,}")
            print("\nNext steps:")
            print("  1. Run demo: python demo/run_complete_demo.py")
            print("  2. Or try manual queries:")
            print(f"     curl '{self.solr_url}/{self.collection_name}/select?q=laptop&rows=5'")
            print("="*60)
            
        except Exception as e:
            logger.error(f"‚ùå Error generating summary: {e}")


def main():
    """Main function to load data into Solr."""
    print("üöÄ Solr Data Loader")
    print("==================\n")
    
    loader = SolrDataLoader()
    
    try:
        # Pre-flight checks
        if not loader.check_solr_connection():
            sys.exit(1)
        
        if not loader.check_collection_exists():
            sys.exit(1)
        
        # Clear existing data
        if not loader.clear_existing_data():
            sys.exit(1)
        
        # Load products
        if not loader.load_products():
            sys.exit(1)
        
        # Verify loading
        if not loader.verify_data_loaded():
            sys.exit(1)
        
        # Create demo files
        loader.create_demo_queries_file()
        
        # Show summary
        loader.show_loading_summary()
        
        print("\n‚úÖ Data loading completed successfully!")
        
    except KeyboardInterrupt:
        logger.info("\nüõë Loading interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"‚ùå Unexpected error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
