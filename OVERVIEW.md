# Initial Requirements

Design a set of AI agents or an agentic framework that can optimize Solr queries. Because this is an open-ended problem it should be able to reference a log of configurations, parameters, and techniques that it has already tried. It should have the ability to understand document schemas, field types, and analysis chains. It should have deep understanding of each query feature of Solr including streaming expressions, pivot facets, minimum-should-match, and all the rest. To do this it will reference a named set of queries, a named corpus, and a named set of query-document judgments. Each iteration will compare the current results against a named previous iteration, defaulting to the last run. The agent or agents will have the ability to examine individual results between iterations and explain differences.

## Request for Clarification

To design this agentic framework effectively, could you please clarify a few key details:

1. What are the primary optimization goals? (e.g., precision, recall, MRR, latency, query cost)
2. What is the nature of the query-document relevance judgments? Are they graded relevance (e.g., 0-3) or binary (relevant/not relevant)?
3. What kind of named corpora and schemas are we working with (e.g., legal docs, e-commerce, academic articles)?
4. What Solr version or features are in scope—e.g., SolrCloud, Fusion, or standalone Solr?
5. Should this framework integrate with external tools (e.g., Learning to Rank models, Jupyter, Kibana)?

Once I have these details, I’ll get started on a robust research-driven design.

## Clarifications

1. The primary metric should be specified by the end user, with a selection of nDCG, DCG, ERR, MRR, precision, recall, etc. The depth to inspect (e.g. DCG@10) should also be specified by the user.
2. The judgment scale is also user-defined and should match the expected domain of the metrics they choose.
3. The nature of the corpus is undefined.
4. SolrCloud
5. If there is a clear leading open source platform that would make dashboards, configuration, and visualization easier then yes. Otherwise it's enough to expose an API.

## Response

Great. I’ll design an agentic framework for SolrCloud query optimization that can:

* Optimize Solr queries using a user-specified relevance metric (e.g., nDCG\@10, MRR, etc.) and judgment scale.
* Track previous iterations with named queries, corpora, and relevance judgments.
* Understand and leverage Solr-specific features like streaming expressions, pivot facets, field types, analysis chains, etc.
* Log previously attempted configurations, parameters, and techniques.
* Compare result sets between runs and explain differences.
* Optionally integrate with a leading open source visualization or dashboard tool if available; otherwise, expose an API.

I’ll explore existing tools, libraries, and architectures to support this and return a detailed design plan.

## Agentic Framework for Query Optimization in Apache SolrCloud

### Introduction

Optimizing search query relevance in Apache SolrCloud requires a systematic approach that can iterate on query configurations and improve result quality according to objective metrics. We propose a modular **agentic framework** – a collection of AI-driven agents – that collaboratively tune Solr queries and configurations to maximize a chosen relevance metric. The system operates on any corpus indexed in SolrCloud (domain-agnostic) and uses **user-defined relevance judgments** (graded ratings of query-document relevance) as ground truth. By leveraging SolrCloud’s advanced capabilities (fielded search, analysis chains, faceting, Learning-to-Rank, etc.), the framework can intelligently modify queries or ranking parameters, evaluate outcomes with metrics like NDCG or MRR, and explain the changes in results. Experiments are logged with full history, allowing comparisons between iterations and integration with external dashboards for monitoring progress. The following design details the architecture, components, and technologies for this framework, ensuring each of the user’s requirements is met.

### Modular Architecture and Agents

The framework is organized into several specialized **agents** (or modules), each responsible for a part of the optimization loop. This modular design promotes clarity and extensibility, as new strategies or tools can be integrated by adding or updating agents. The primary agents/modules include:

* **Experiment Manager (Coordinator)** – Orchestrates the end-to-end process. It reads user input (selected metric, relevance judgments, corpus/collection name, etc.), initializes the experiment, and coordinates the other agents. It maintains the state of each iteration and ensures that by default each new run is compared against the previous one (or a specified baseline run).
* **Query Optimization Agents** – A collaborative team of specialized AI agents that work together to optimize different aspects of search relevance. Each agent is implemented using the **Model Context Protocol (MCP)** with **Pydantic AI**, focusing on a specific domain of expertise:

  * *Schema Analysis Agent* – Analyzes document schemas and field configurations to recommend optimal field boost weights and suggest schema modifications for better searchability.
  
  * *Analysis Chain Agent* – Evaluates and optimizes tokenization, stemming, and other text analysis processes. Suggests synonym expansions, stopword adjustments, and analyzer configurations.
  
  * *Query Rewriting Agent* – Reformulates queries to improve relevance, expands queries with related terms, and suggests query structure changes.
  
  * *Parameter Tuning Agent* – Optimizes DisMax/eDisMax parameters (qf, pf, tie), tunes minimum_should_match settings, and adjusts boosting factors and function queries.
  
  * *Learning-to-Rank Agent* – Creates and optimizes machine learning ranking models, evaluates feature importance, and suggests feature engineering improvements.

  These agents are coordinated by a *Query Optimization Orchestrator* which integrates their recommendations into a coherent optimization strategy. The MCP implementation ensures each agent can understand context from previous iterations and learn from past optimization attempts, providing increasingly effective suggestions over time. This multi-agent approach ensures deep specialization while maintaining both **schema-awareness** and **continuous learning** in the optimization process.
* **Solr Query Execution Agent** – Interfaces with the Apache SolrCloud cluster to run queries and retrieve results. It uses Solr’s APIs (e.g. SolrJ in Java or HTTP/REST calls via a Python Solr client) to issue the search requests with the current query parameters. This agent can also engage Solr’s advanced query features as needed: for instance, it can retrieve **pivot facet** results (to analyze how results break down across multiple fields), use **streaming expressions** for complex result processing, or vary the `edismax` parser settings like **minimum-should-match**. All such Solr capabilities are available to be exploited in the query request. The use of these features in combination can improve search relevance, as demonstrated by prior work – for example, combining Solr’s Learning-to-Rank reranker with Streaming Expressions provides powerful ways to boost result quality.
* **Evaluation & Metrics Agent** – After the Solr query results are obtained, this agent evaluates the ranking against the **user-provided relevance judgments**. It computes the chosen metric (e.g. nDCG\@10, ERR\@5, MRR, Precision\@10, Recall\@100, etc.), taking into account the specified depth *k* and the graded relevance scale from the judgments. The system supports **custom relevance scales**; for instance, if documents are judged on a 3-point scale (0 = not relevant, 1 = partially, 2 = highly relevant), the metrics will treat these values appropriately (metrics like DCG/NDCG naturally accommodate graded relevance). If a binary metric like Precision is chosen, the agent will interpret any judgment above a threshold (e.g. >0) as “relevant” for that calculation. Under the hood, this module can leverage existing libraries or frameworks for accuracy and efficiency. For example, the **Rated Ranking Evaluator (RRE)** library or **pytrec\_eval** can be used to compute standard IR metrics given the result rankings and judgment lists. These tools support a wide range of metrics and allow configuration of at-value cutoffs (e.g. Precision\@10 or nDCG\@10). The metrics agent produces both per-query metrics and an aggregate (mean or weighted average) across all test queries.
* **Logging & Experiment Tracking Agent** – Every experiment iteration is logged in detail. This agent records the query parameters/configuration used, the resulting metric scores (overall and per query), and any notable changes from the previous iteration. The log is structured (for example, as JSON entries or database records) to facilitate later analysis and visualization. Crucially, the log includes a **version or iteration identifier**. By default, each new run is tagged as version *n+1* relative to the last, and the system can reference prior runs by these IDs or names. The log enables a **full history of tries** and their outcomes, much like how the Quepid tool snapshots test runs for relevance tuning. This historical tracking supports backtracking to earlier configurations or branching experiment paths. It also notes the user’s chosen metric and any custom settings for that run (such as a different judgment scale or subset of queries used).
* **Comparison & Analysis Agent** – This agent compares the results of the current run to a baseline (by default, the immediate previous run, though the user can specify any earlier run as a baseline). The comparison happens at multiple levels:

  * *Metric level:* It computes the differences in the primary metric (and potentially secondary metrics) between iterations. For example, it might report that nDCG\@10 increased from 0.45 to 0.52 (+0.07) compared to the last run, or that MRR declined slightly. The agent identifies whether changes are statistically significant if applicable (though for deterministic comparisons with the same queries, it’s usually a direct difference). The Rated Ranking Evaluator framework, for instance, inherently keeps track of metrics for each version and can even provide the delta/trend between subsequent versions. This gives immediate feedback on whether a particular change improved or hurt overall relevance.
  * *Query level:* The agent drills down to see which specific queries got better or worse. It might list queries where the metric (e.g. AP or nDCG) improved, and those where it declined, highlighting potential trade-offs (one query’s results improved at the expense of another’s). This helps pinpoint if an optimization is overfitting to certain queries or genuinely lifting all boats.
  * *Document/result level:* For individual query results, the agent analyzes how the ranking of specific documents changed from the baseline. It identifies documents that moved up or down in rank or new documents that appeared in the top *k* (and which ones dropped out). To **explain why rankings changed**, the agent utilizes Solr’s **explanation** details. Solr can provide a scoring explanation for each document (using `debugQuery=true`), which includes term frequency contributions, field boosts, and any additive scores from boosts or functions. The analysis agent can parse these explanations (using a library or tool like **Splainer** which is designed to interpret Solr/Elasticsearch relevance explanations) to highlight differences. For example, if a document gained rank because a certain field boost was increased, the explanation would show a higher contribution from that field’s term match in the new run. Conversely, a document that fell in rank might have lost points because a query clause was made stricter (perhaps due to `minimum_should_match` increasing). By comparing explain outputs, the agent can articulate reasons such as “**Document X** moved from rank 5 to 2 because the boost on the `title` field was increased – Document X has strong title term matches, which now contribute more to its score.” Such granular analysis is aided by tools like Splainer, which provide *deeper details about individual relevancy scores*, complementing the high-level view of metrics.

All agents communicate through the Experiment Manager, which passes along the necessary data (e.g. the Query Tuning agent produces a new query config, the Execution agent returns results, the Evaluation agent returns metric scores, etc.). This design allows **swapping modules** if needed – for example, one could replace the Query Tuning logic with a more advanced algorithm (like a genetic algorithm or reinforcement learning agent) without affecting the other parts.

### Metric Optimization and Custom Relevance Scales

**Metric Configuration:** The framework lets the user specify a **primary optimization metric** and parameters like cutoff depth. This could be a standard ranking metric such as **nDCG**, **DCG**, **MRR** (Mean Reciprocal Rank), **Precision\@K**, **Recall\@K**, or more novel ones like **ERR** (Expected Reciprocal Rank). The user can indicate the depth *K* for metrics that consider top-K results (e.g. nDCG\@10, Precision\@5, etc.). Internally, the Evaluation agent will compute exactly that metric. Notably, many of these metrics are *graded relevance* metrics – nDCG and DCG consider varying degrees of relevance, and ERR is an extension of reciprocal rank that supports graded relevance. This means the system doesn’t force judgments into binary relevant/non-relevant categories unless the metric itself is binary (like Precision or Recall).

**Custom Relevance Judgments:** Users can provide relevance judgments on any numeric scale (for example: 0–1 binary, 0–3, 1–5 stars, etc.). The framework will normalize and utilize these in metric computations appropriately. For graded metrics (DCG/nDCG, ERR), the exact judgment values are used as the “gain” or relevance grade for each document. For binary metrics (Precision, Recall, MAP), the system can treat the highest values as “relevant” or use a threshold (configurable if, say, 3–5 on a 5-point scale should count as relevant). The flexibility in judgment scale ensures compatibility with various relevance rating systems (e.g. some domains might use a three-level relevance: *Not Relevant*, *Relevant*, *Highly Relevant* – these can be encoded as 0,1,2 and the metrics will reflect those differences).

**Evaluation Library:** We recommend using existing libraries for metric computation to ensure accuracy and to avoid reinventing the wheel. In a Java-based implementation, the **Rated Ranking Evaluator (RRE)** core library can be invoked – RRE supports all the key IR metrics out-of-the-box and even allows plugging in new metrics if needed. If the framework is implemented in Python, **`pytrec_eval`** (an open-source Python wrapper of TREC’s evaluation toolkit) or the newer **`ir_measures`** library can be used to compute metrics like nDCG, ERR, etc., directly from the relevance judgments. These libraries will handle the math of discounting, normalization, etc., given the judgment scale. The user’s choice of metric and depth is simply passed to the library and the result is returned.

Finally, while one primary metric is optimized, the system can track **secondary metrics** as well for informational purposes. For instance, if nDCG\@10 is primary, the system might also report changes in Recall\@100 or Precision\@10, so the user understands broader impacts (e.g. relevance improved but recall dropped). This multi-metric view can be logged each iteration, though the optimization agent will focus on the primary metric as the objective.

### SolrCloud Integration and Advanced Features

This framework is built specifically to work with **Apache SolrCloud**, meaning it understands Solr’s schema-driven architecture and query capabilities. The integration points and advanced feature usage include:

* **Schema & Field Awareness:** The system can query Solr’s Schema API or use the cluster’s schema configurations (accessible via ZooKeeper in SolrCloud) to understand the index fields, their types, and any field-specific query-time configurations. This allows the Query Tuning agent to make informed decisions, like boosting certain fields or constructing fielded queries. For example, if the schema defines a `title` field (text) and a `tags` field (strings or text), the agent might decide to search both but weight `title` higher. Field type knowledge (e.g. a field is numeric vs text) is useful – numeric fields might be used in range queries or function queries, while text fields go through analyzers. The agent can also be aware of **analysis chains** configured for each field (tokenizers, stemmers, synonym filters, etc.). If, say, a synonym expansion filter is present for a field, the agent knows Solr is already handling synonyms at query time, so it might avoid redundant query expansion on its side. Conversely, if analysis is very strict, the agent might suggest using Solr’s **Query parsers** (e.g. `edismax`) with options like `pf` (phrase boosts) or `mm` (minimum-should-match) to better handle loose queries.

* **Query Execution via SolrCloud API:** The Execution agent ensures queries are distributed to the SolrCloud cluster. Using Solr’s standard REST API, it sends requests including all necessary parameters (selected by the Tuning agent). This includes the collection (or core) name representing the **corpus** – our system can handle multiple corpora by simply pointing to the respective collection. The user can reference a **named corpus** which the framework maps to the correct SolrCloud collection. The agent handles any necessary encoding of queries and retrieves the JSON or XML response from Solr, which contains the ranked documents and their scores.

* **Advanced Query Parameters:** The system is capable of leveraging Solr’s rich query parameters to improve relevance. A few examples:

  * **DisMax/EDisMax parameters:** The agent can adjust parameters like `qf` (query fields and their weights), `pf` (phrase boost fields for multi-term queries), and `tie` (tie-breaker for multiple fields). It can also manipulate **`mm` (Minimum Should Match)**, which controls how many terms of a multi-term query must match. Tuning `mm` is a known technique to balance precision and recall; the agent can experiment with stricter or looser `mm` settings (e.g. requiring 100% of terms vs 70% of terms to match) and see which yields better metrics.
  * **Function Queries and Boosting:** The agent might incorporate Solr function queries or boosts to adjust ranking. For instance, if recency (date) or popularity (view count, etc.) is relevant, it can include a `bf` (boost function) or sort by a function of multiple fields. These can be part of the iteration process (e.g. test a hypothesis that newer documents should be ranked higher by adding a reciprocal of age as a boost).
  * **Pivot Facets and Result Analysis:** While pivot facets (i.e. multi-level faceting) are not directly a ranking tool, the agent can use pivot facets to analyze results. For example, after a query, it could request a pivot facet on `category` and `author` fields to see how results are distributed. This might reveal, say, that the top results are dominated by one category. If that’s undesirable, the agent could attempt to diversify results in the next iteration (perhaps by incorporating a query filter or a diversity re-ranking strategy). Pivot facets essentially provide a **high-level breakdown of search results** on multiple dimensions, which can feed into the agent’s decision-making.
  * **Streaming Expressions:** Solr’s streaming expressions allow for complex server-side processing of query results and even intermixing of multiple queries or collections. Our framework can utilize streaming expressions to, for instance, **aggregate statistics** needed for tuning or to perform **offline learning to rank model training**. One potential use is to gather training data for a Learning-to-Rank model: a streaming expression could pull a sample of queries and documents with features and join with relevance labels to feed an ML algorithm. Another use is performing **A/B testing** comparisons: using `select` streaming source to run two query parameter sets and then a `join` or `merge` to compare scores document by document. Streaming expressions essentially give the agents a mini-framework *within Solr* for data processing, which can reduce the need to export large result sets out of Solr for analysis. Because these features are part of Solr, the agent can seamlessly integrate them to enhance the optimization process.
  * **Learning to Rank (LTR) Integration:** Apache Solr includes a Learning-to-Rank module that can apply machine-learned models to re-score the top N results of a query. Our framework can optionally incorporate an agent that trains an LTR model using the provided relevance judgments. This could be an **offline training agent** that uses gradient boosting or neural rankers to optimize an objective correlated with the chosen metric (e.g. a LambdaMART model optimizing NDCG). Once trained, the model can be uploaded to Solr and the Query Execution agent can apply it (via `rq` parameter in Solr queries) to rerank results. The iterative framework could treat this as one of the techniques to try – for example, after some manual tuning, an iteration might involve “Train an LTR model on the judgments and use it”. The results from that can be evaluated and compared to the manual tuning approach. By supporting LTR, the framework leverages state-of-the-art ML to potentially achieve large relevance improvements.
  * **Other Solr Features:** The design does not preclude using any Solr capability as part of optimization. This includes **faceted navigation** (using facet results to perhaps bias results towards facets that the user cares about), **query elevation/rules** (Solr’s feature to force certain documents to appear for certain queries – the agent could test if “curated” elevations improve user satisfaction metrics if such data is available), and more. The key is that the agentic framework is aware of these tools and can deploy them when appropriate.

All these features are employed while keeping the solution **corpus-agnostic**. Whether the corpus is e-commerce products, news articles, or research papers, the framework works the same way – by inspecting schema and using generic relevance signals (text matches, numeric boosts, etc.). The user does not need to hardcode any domain-specific logic; however, the system is extensible such that if a particular corpus type benefits from a special technique (say, handling synonyms in medical terminology), the Query Tuning agent can be extended with a plugin for that case.

### Iterative Experimentation and Logging

At the heart of the framework is an **iterative experiment loop**. The user (or an automated process) triggers successive iterations of query optimization, and the system keeps track of each attempt:

1. **Initial Setup:** The user specifies the dataset (corpus/collection name or a previously defined “named corpus”), the set of test queries (which could also be referenced by a name if predefined), and the relevance judgments (often called “qrels” in IR terminology) for those query-document pairs. These can be loaded from files or a database – for example, a user might have a CSV or TREC-format file of judgments. The user also selects the primary metric (and any secondary metrics to observe) and the optimization strategy (the user might choose a manual mode or let an AI strategy run automatically). The Experiment Manager records this setup in the log, marking it as **Iteration 0** or **Baseline**. If an existing baseline run is available (e.g. current production Solr settings), the system can import those results as the baseline.

2. **Iteration Execution:** For each iteration, the Query Tuning agent generates a new configuration. This could be based on the previous iteration’s analysis (for example, if last run showed poor recall, it might lower the `mm` requirement or boost a recall-oriented field). The Execution agent runs all test queries against Solr with this new configuration and collects results. Then the Evaluation agent computes metrics by comparing results to judgments. The Logging agent stores the configuration and metrics. Immediately, the Comparison agent compares it with the baseline (prior iteration) and produces a diff report (which is also logged). All these steps are automated, so an “iteration” can be executed by a single command or user action, and the system will produce the outcome data.

3. **Experiment Log Structure:** The log contains each iteration’s essential data. In practice, this could be a structured file or an experimental database. Key fields logged per iteration include:

   * **Iteration ID/Name:** e.g., “Baseline”, “Iter1”, “Iter2”, or semantic names if the user provides (like “BoostTitle\_x2” to remember what was changed).
   * **Date/Time and User:** when and by whom the iteration was run (useful for collaborative teams).
   * **Configuration Details:** what was tried in that iteration – e.g. “title boost = 2.0, body boost = 1.0; mm=2<->3 (i.e., 2 clauses must match out of 3); using LTR model = off; query parser = EDisMax; …”. Only the differences from baseline might be recorded for brevity, but storing the full config ensures reproducibility.
   * **Metric Results:** the value of the primary metric (and secondary metrics) for this run. Often this will be broken down by query group or other aggregate if the queries are categorized. For example, RRE organizes queries into groups and can report metrics per group; our log could similarly record per-group metrics if relevant, or at least overall metrics.
   * **Comparison to Baseline:** either store absolute differences or just rely on the Comparison agent to produce a report on the fly. RRE, for instance, encourages logging multiple versions and will compute the delta between them. Our system can explicitly log “Δ Metric vs last: +0.07” etc., to make trend tracking easier.
   * **Agent Notes:** any observations the Analysis agent had. For example, it might log “Noticed query ‘X’ still has low precision, maybe need to add synonyms for term Y” as a suggestion for the next iteration (if the system has such heuristic suggestions built-in). It might also log which documents swapped in ranking for notable queries.

   By maintaining this rich log, the framework supports traceability and learning from past attempts. It aligns with the concept of **test-driven relevancy tuning** where each change is recorded and measured. If the user wonders “what changed between iteration 5 and 2 that improved nDCG by 10 points?”, they can inspect the log and diff those iterations.

4. **Referencing Named Entities:** The design supports user-friendly referencing of data. Instead of always specifying raw file paths or IDs, the user can register names – e.g., call a corpus “News2024” which corresponds to a Solr collection, or a query set “HolidayQueries” with its judgments. The system can maintain a catalog of these references. Then a command might be “optimize News2024 using HolidayQueries metric=nDCG\@10” and the framework will load the appropriate collection and judgments. This makes it easier to reuse the framework across projects or corpora. All references are resolved at runtime by the Experiment Manager, which fetches the actual connection info for the Solr collection and loads the correct judgments file or database entries for the queries.

Throughout this iterative process, the framework is **self-documenting via its log**. This is invaluable for collaboration (multiple engineers can see what has been tried) and for avoiding repetition. Moreover, the log and versioning approach allows **comparing any two iterations** (not just consecutive). By default, comparisons are with the immediate previous run for convenience, but the user may request a comparison of, say, the current run vs the original baseline to see cumulative improvement, or between two different strategies (perhaps one branch of tuning vs another).

### Performance Comparison and Result Analysis

Comparing performance across iterations is a core feature of the framework. Immediately after each run, the system provides a **comparison report** highlighting differences to the previous iteration (or a selected baseline). This serves as feedback for the next tuning decision and also as a validation that a change had the intended effect. The comparison and analysis consist of:

* **Overall Metric Change:** A summary stating how the primary metric moved. For example: “Iteration 5 vs Iteration 4: **NDCG\@10 = 0.527 (+0.032)**” – meaning a 3.2 point improvement in NDCG at 10 results. If multiple metrics are tracked, those are listed too (e.g. “Recall\@100 = 0.85 (-0.01)”, indicating a slight recall drop). This informs if the change was beneficial on balance.

* **Per-Query Analysis:** The system can provide a breakdown of metric changes per query (or per query group, if queries are categorized). It might list a few queries that saw the biggest improvement and the few that saw declines. For example: “Query ‘camera lens’ – NDCG\@10 improved from 0.40 to 0.55 (several highly relevant items moved into the top 10); Query ‘camera bag’ – NDCG\@10 dropped from 0.70 to 0.60 (a formerly top-ranked relevant doc fell to rank 12)”. Such detail helps diagnose if certain query types are suffering. Perhaps the optimization over-boosted a field that matters for some queries but hurts others. The user (or the autonomous agent) can then address these specific regressions in subsequent iterations (for instance, if ‘camera bag’ lost a relevant result because it was filtered out, the next iteration might relax that filter or add a synonym).

* **Document-Level Ranking Changes:** The Analysis agent produces a **diff of rankings** for each query, focusing on the top results. This can be represented as, for example, a table or list per query:

  * Documents newly appearing in the top K (and their relevance scores if judged).
  * Documents that were in the previous top K but are now missing (and whether they were relevant).
  * Rank shifts for persistent documents (e.g. “Doc ID 1234 went from rank 8 to rank 3”).

  Alongside these changes, the agent provides **explanations** for *why* key documents moved. This is enabled by examining Solr’s scoring breakdown. The agent might output statements like:

  * “Document 1234 gained rank because its field `title` contains the query terms more exactly, and the boost on `title` was increased. The Solr explain shows a higher score contribution from the `title` field in this iteration, boosting it above other docs.”
  * “Document 5678 dropped out of the top 10. Explanation indicates its score was penalized by the new **minimum-should-match** setting – it only matched 3 of 5 query terms, and the new policy requires 4, so it was not considered as strongly.”

  Such explanations leverage either direct parsing of the `debugQuery` output or external libraries like Splainer to translate the low-level scoring details into human-readable reasons. The ability to **pinpoint the cause of a ranking change down to the document level** is a standout feature. As noted earlier, tools like Splainer have been used in practice to give deeper insight into individual scores, and our framework automates that comparison between runs.

* **Why Rankings Changed:** In summary, the system doesn’t just show that things changed, but also **why**. By having access to both iterations’ configurations and the internal scoring for documents, it can infer the reasons behind changes:

  * If a field boost changed, any document heavy on that field might surge in rank – the agent will correlate that.
  * If a new synonym expansion was enabled, the agent might notice that certain documents now match an extra query term (the synonym), hence got a higher score.
  * If an LTR model was introduced, the agent can inspect which features likely caused a doc to move (this requires analyzing the model’s feature weights and the doc’s feature values; the framework could output the top contributing features for a reranked document). For example, “LTR model gave Doc 42 a boost due to a strong click-through feature, moving it above others.”

The framework essentially provides an **audit trail and diagnostic for ranking**, which is crucial for trust. When an automated agent is making changes, the team will want to know why a particular change helped or hurt – this component ensures that every change can be rationalized with evidence.

### Integration with Visualization and Dashboard Tools

To make the experimentation process transparent and accessible, the framework can integrate with popular open-source visualization and logging platforms. The goal is to allow users (and stakeholders like product managers or search relevance engineers) to monitor progress over time and dig into specific results without requiring them to read raw log files or console output. Two approaches are considered:

1. **Direct Dashboard Integration:** The experiment log can be output in a format that is easily consumed by visualization tools. For instance, each iteration’s results could be sent to an Elasticsearch index (since the user is already using Solr, they might have ELK stack available, or we could even use Solr itself as the backend for logs). Then a **Kibana** dashboard could be set up to plot metrics over time, show tables of queries with their scores, etc. Kibana is a leading open-source visualization tool that would allow interactive filtering (e.g., view only a subset of queries or compare specific runs). Alternatively, a **Grafana** dashboard can be used; Grafana could pull the data from a time-series database or even directly from the log file if structured (Grafana supports JSON data sources or SQL databases). Grafana is excellent for graphing metrics like NDCG over successive runs, or even setting up alerts if a metric drops. Another option is **Apache Superset** or **Metabase** – open-source BI tools that can create charts and tables from relational data. The framework could save the log to a small SQLite or PostgreSQL database, and Superset/Metabase can connect to it to build a dashboard of experiments.

2. **Experiment Tracking Platforms:** Since this optimization process is analogous to an ML experimentation process, we can leverage ML experiment tracking tools. **MLflow** is an open-source platform that provides APIs to log parameters, metrics, and artifacts for each run. Our framework could create an MLflow experiment for a given corpus, logging each iteration as a “run” with parameters (the query config) and metrics (nDCG, etc.). MLflow’s web UI would then allow the user to compare runs side by side, visualize metric improvements, and even retrieve artifacts like detailed reports for each run. Another modern tool is **AimStack (Aim)**, an open-source experiment tracker that has a friendly UI for comparing runs and can log not just metrics but also textual logs and charts. Using such a tool saves development effort on building a custom UI; we’d just push data to it. These platforms also allow tagging or naming runs, which fits with our named iteration concept.

If a more search-specific dashboard is desired, one could integrate with existing relevance tuning UIs: for example, **Quepid** (which is open-source) could potentially be fed the relevance judgments and results of each run to visualize the search results for each query and how they change. Quepid is normally interactive, but perhaps our system can output data in a format Quepid understands. In absence of a direct integration, the framework’s **clean API** ensures that any custom visualization can be built on top. The API could allow retrieving:

* The list of experiments (with their IDs, timestamps, descriptions).
* The detailed results of a specific experiment (metrics, per-query outcomes, etc.).
* Pairwise comparison data between two experiments (maybe providing a structured diff, including which docs moved ranks).

With such an API, a lightweight web application could be built if needed, or the data could be plugged into tools like Jupyter notebooks for ad-hoc analysis (e.g., using Python to generate Matplotlib/Plotly charts of performance over time). The key is that the data is readily accessible and well-structured.

In summary, while the core framework runs as a backend process, it produces data that can be visualized using leading open-source tools. If none of those tools fit the user’s needs, the provided API and logs allow easy integration into any custom dashboard or reporting solution.

### Key Technologies and Libraries

Implementing this agent-based framework will benefit from various technologies and libraries:

* **Apache SolrJ or Solr HTTP API**: For interacting with SolrCloud, SolrJ (the official Java client) is a robust choice if using Java – it handles load-balanced queries, Zookeeper cloud discovery, and building query requests with Solr’s query parameters. If using Python or another language, a simple HTTP client or library like `pysolr` (Python Solr client) can be used to submit queries and retrieve results. The SolrCloud JSON API will be used extensively for queries, schema requests, and possibly for sending streaming expression commands.

* **Relevance Evaluation Libraries**: As discussed, libraries like **pytrec\_eval** (for Python) or **RRE (Rated Ranking Evaluator)** for Java can be utilized to compute metrics such as NDCG, DCG, Precision, Recall, ERR, etc. These save time and ensure our metrics match standard definitions. RRE in particular is designed for Solr/Elasticsearch and provides a framework where metrics are *configurable/extendable* and can be aggregated at different levels. It also inherently supports tracking multiple experiment versions and computing deltas, which we can either use directly or mirror in our own logic.

* **Logging and Experiment Tracking**: We can use a combination of structured logging and tracking tools. For example, using a JSON log file for each run or an embedded database (like SQLite) to record experiments. On top of that, integrating **MLflow** for experiment tracking provides a ready-made solution for logging parameters and metrics with minimal code. MLflow’s experiment UI will list runs and show metrics. If a more search-specific tool is preferred, the open-source **Quepid** platform can serve as a UI by importing our results – Quepid was built for test-driven search relevance tuning and can display queries, judgments, and metrics in a user-friendly way. Another simple approach is logging to a CSV which can be imported to Excel or other tools, but given the iterative and collaborative nature, a proper tracking system is better.

* **Analysis and Explanation Tools**: For parsing Solr’s `debugQuery` output, the **Splainer** library (by OSC) can be extremely useful. Splainer is available as a JavaScript library (and as a web service UI) that takes Solr/Elasticsearch raw explanations and breaks them into a structured object showing each term’s contribution. While Splainer is not a traditional API service, we could incorporate a similar logic by either calling Splainer’s backend or by using Solr’s own explain in JSON mode. Solr can output explanations in a structured JSON format if requested (`debug=true&debug.explain.structured=true`), which might be easier for our Python/Java agents to parse than the default plain text explanation. Using this, our Analysis agent can programmatically traverse the explanation tree to find what changed between runs. Additionally, for advanced analysis, one might integrate Python NLP libraries (like **NLTK or spaCy**) if analyzing query terms or looking for synonyms, but since Solr’s analysis handles much of that, it might not be necessary to do outside Solr.

* **Machine Learning Libraries**: If we implement an AI-driven tuning agent (beyond heuristic rules), libraries like **scikit-learn** or **LightGBM/XGBoost** (for learning to rank models) could be used. For example, LightGBM has a learning-to-rank mode that could train an LTR model from the judgments. If using reinforcement learning to try query tweaks, frameworks like **Ray RLlib** or even custom bandit algorithms might be applied. These would observe the metric score as a reward and try new configurations accordingly. This is an area for future enhancement – initial implementation might stick to a simpler grid-search or rule-based approach for query tuning, given the complexity of treating Solr as an RL environment.

* **Dashboard/Visualization**: Integration to **Kibana** would assume an ELK stack – if the user already has Elasticsearch, pushing logs there is an option. Otherwise, a simpler route for graphing is **Grafana** with a JSON data source or reading from a DB. **Apache Superset** (incubating) can connect to our experiment DB and create rich charts and data explorations with minimal coding. All these are open-source. If using MLflow for tracking, it includes a basic UI; AimStack is another open source tool with a nifty UI for comparing experiment runs.

By combining these technologies, we ensure the framework is not built from scratch entirely, but rather stands on the shoulders of proven tools in the search and ML community. This accelerates development and makes the system more reliable.

### Conclusion

In this design, we outlined an **agentic, modular framework** for optimizing Apache SolrCloud query performance. The system empowers users to specify their own success metric (from precision to nDCG with custom depths and judgment scales) and then automatically iterates to improve that metric. It does so in a generalizable way, working across different corpora and domains by utilizing Solr’s schema information and rich querying features. Each component – from query tuning, execution, and evaluation to logging and analysis – is cleanly separated as an “agent” with a clear role, which makes the framework extensible and maintainable.

Critically, the framework not only tunes relevance but also **explains and documents** the tuning process. Each experiment is logged with full context, and differences between iterations are analyzed down to the document level, providing valuable insights into why certain changes have the effect they do. This transparency builds trust in the system’s recommendations and allows human engineers to learn from the AI agents’ trials. By integrating with open-source visualization tools and providing an API, the framework ensures that its insights can be easily consumed and acted upon, whether in a research setting or a production search improvement cycle.

Together, these design elements meet all the specified requirements. The result is a powerful loop of continuous improvement for search relevance: propose an enhancement, test it with objective metrics, explain the outcome, log the experience, and then repeat – driving SolrCloud-based search systems toward optimal results for their users.

**Sources:**

* OpenSource Connections – *Quepid: the relevance workbench* (supports gathering judgments, computing NDCG, and tracking search tuning history).
* Sease Ltd. – *Rated Ranking Evaluator (RRE)* (open-source search evaluation tool supporting metrics like Precision, ERR, nDCG and comparison of multiple versions of a search configuration).
* Kibo Commerce Docs – *Quepid & Splainer integration* (using Quepid for overall relevance testing and Splainer for detailed explanation of Solr relevance scores).
* Wikipedia – *Discounted Cumulative Gain* (nDCG uses a graded relevance scale for evaluating ranked results, allowing custom judgment values to be utilized in computing gain).
* Sematext Blog – *Solr Learning to Rank and Streaming Expressions* (combining Solr’s built-in ML reranking with streaming expressions and other Solr features can improve search relevance).
